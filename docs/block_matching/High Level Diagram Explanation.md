# Document Files

Documents are typically stored in "document streams", represented as large files containing several documents along with their metadata. In order to ingest these documents into the index, a parser is required to read out individual documents.

# Document Readers

Document readers are found in the `document_readers` folder. Each document reader implements the `reader_interface`, which represents a pointer to a document in the document stream. This pointer can be "dereferenced" to get the contents and url of the document being pointed to, and the pointer can be advanced to the next document.

However, there are usually several document streams in an entire document collection, so any document reader implementing `reader_interface` should be able to advance through documents in a single stream, and move to a different stream when the current stream has been exhausted.

# doc_analyzer

Once a document has been parsed out, it's passed to the document processing stage. The specifics of this stage varies between the different approaches to documenting updating, but the final output is a list of "external postings" to be inserted into the index, along with any other metadata to be inserted into various different data structures.

# Document Store

The document store abstractly is a tuple-store containing document metadata for use in query processing and other index maintenance processes. Concretely, it stores two maps and three global variables: `nextid` represents the next document ID to be assigned, `avgdoclen` represents the average document length, and `doccount` represents the number of documents indexed.

The first map maps a document's url to `docID` representing its id, `doc` representing the document contents, `doclength` representing the document's length in words, `maxfragID` representing the largest fragment ID in use for that document, and `timestamp` representing when the document was indexed.

The second map maps a `docID` to a `url`, which is useful when a document's metadata is required but only the `docID` is known.

# Translation Table

The translation store is another tuple-store which contains the series of block translations for each document, which is also outputted by the `doc_analyzer`

# Dynamic Index

The dynamic index acts as an intermediate buffer between the postings generated by the `doc_analyzer` and the static index on disk. Postings are stored into the dynamic index until it reaches a certain size (measured by the number of postings currently in the index), then the index is written to disk.

Note that the postings output by the `doc_analyzer` are slightly different than the postings stored by the dynamic index (`externalpostings.h` vs `posting.hpp`); this is because terms are stored as numbers in the index to reduce size, and the `doc_analyzer` doesn't have knowledge about this term-to-number mapping. Future work could involve investigating whether this separation of posting definitions is required

# Static Index

When the dynamic index has reached a certain size, the index is written to disk as a single file. Separate indexes are merged using a logarithmic-merge strategy, which is detailed in the "final-report" paper.

# Query Processing

Non-positional query processing is performed similarly to query processing on traditional indexes. The only difference is that there are many indexes that need to be searched for relevant terms (the dynamic index along with all of the static indexes on disk). Thus the traditional `list_pointer` query primitive is split into two layers: a low-level `list_pointer` that operates on individual indexes, and a high-level `list_pointer` that operates on a collection of low-level `list_pointer`'s. The semantics of the high-level `list_pointer` are the same as the traditional `list_pointer`, but under the hood it uses the low-level `list_pointer`'s to search over all of the posting lists in a uniform fashion.

Positional query processing still needs to be investigated.